/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements. See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License. You may obtain a copy of the License at
 * http://www.apache.org/licenses/LICENSE-2.0
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.nifi.processors.network;

import static org.apache.nifi.processors.network.parser.Netflowv5Parser.getHeaderFields;
import static org.apache.nifi.processors.network.parser.Netflowv5Parser.getRecordFields;
import static org.apache.nifi.processors.network.parser.Netflowv5Template.getTemplate;
import static org.apache.nifi.processors.network.parser.Netflowv5Template.templateID;

import java.io.BufferedOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.OptionalInt;
import java.util.Set;
import java.util.concurrent.atomic.AtomicBoolean;

import org.apache.nifi.annotation.behavior.EventDriven;
import org.apache.nifi.annotation.behavior.InputRequirement;
import org.apache.nifi.annotation.behavior.InputRequirement.Requirement;
import org.apache.nifi.annotation.behavior.ReadsAttribute;
import org.apache.nifi.annotation.behavior.ReadsAttributes;
import org.apache.nifi.annotation.behavior.SideEffectFree;
import org.apache.nifi.annotation.behavior.SupportsBatching;
import org.apache.nifi.annotation.behavior.WritesAttribute;
import org.apache.nifi.annotation.behavior.WritesAttributes;
import org.apache.nifi.annotation.documentation.CapabilityDescription;
import org.apache.nifi.annotation.documentation.Tags;
import org.apache.nifi.annotation.lifecycle.OnScheduled;
import org.apache.nifi.components.PropertyDescriptor;
import org.apache.nifi.flowfile.FlowFile;
import org.apache.nifi.flowfile.attributes.CoreAttributes;
import org.apache.nifi.processor.AbstractProcessor;
import org.apache.nifi.processor.ProcessContext;
import org.apache.nifi.processor.ProcessSession;
import org.apache.nifi.processor.Relationship;
import org.apache.nifi.processor.exception.ProcessException;
import org.apache.nifi.processor.io.InputStreamCallback;
import org.apache.nifi.processor.io.OutputStreamCallback;
import org.apache.nifi.processor.util.StandardValidators;
import org.apache.nifi.processors.network.parser.Netflowv5Parser;
import org.apache.nifi.stream.io.StreamUtils;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ObjectNode;

@EventDriven
@SideEffectFree
@SupportsBatching
@InputRequirement(Requirement.INPUT_REQUIRED)
@Tags({ "network", "netflow", "attributes", "datagram", "v5", "packet", "byte" })
@CapabilityDescription("Parses netflowv5 byte ingest and adds attributes to the FlowFile for headers.")
@ReadsAttributes({ @ReadsAttribute(attribute = "udp.port", description = "Optionally read if packets are received from ListenUDP") })
@WritesAttributes({ @WritesAttribute(attribute = "netflowv5.header.*", description = "The key and value generated by the parsing of the header fields."),
        @WritesAttribute(attribute = "netflowv5.record.*", description = "The key and value generated by the parsing of the record fields."),
        @WritesAttribute(attribute = "templateID", description = "template ID"), @WritesAttribute(attribute = "templateDefinition", description = "Template Definition - one time") })

public class ParseNetflowv5 extends AbstractProcessor {
    // To send the template during first run
    private transient AtomicBoolean isFirstRun = new AtomicBoolean(true);
    private boolean append_raw_to_json = false;
    private String destination;
    // Add mapper
    private static final ObjectMapper mapper = new ObjectMapper();

    public static final String DESTINATION_CONTENT = "flowfile-content";
    public static final String DESTINATION_ATTRIBUTES = "flowfile-attribute";
    public static final PropertyDescriptor FIELDS_DESTINATION = new PropertyDescriptor.Builder().name("FIELDS_DESTINATION").displayName("Parsed fields destination")
            .description("Indicates whether the results of the parser are written " + "to the FlowFile content or a FlowFile attribute; if using " + DESTINATION_ATTRIBUTES
                    + "attribute, fields will be populated as attributes. If set to " + DESTINATION_CONTENT + ", the netflowv5 field will be converted into a flat JSON object.")
            .required(true).allowableValues(DESTINATION_CONTENT, DESTINATION_ATTRIBUTES).defaultValue(DESTINATION_CONTENT).build();

    public static final PropertyDescriptor APPEND_RAW_MESSAGE_TO_JSON = new PropertyDescriptor.Builder().name("APPEND_RAW_MESSAGE_TO_JSON").displayName("Append raw message to JSON")
            .description("When using flowfile-content (i.e. JSON output), add the original netflowv5 message to " + "the resulting JSON object. The original message is added as a string to _raw.")
            .addValidator(StandardValidators.BOOLEAN_VALIDATOR).required(true).defaultValue("true").build();

    static final Relationship REL_FAILURE = new Relationship.Builder().name("failure")
            .description("Any FlowFile that could not be parsed as a netflowv5 message will be transferred to this Relationship without any attributes being added").build();
    static final Relationship REL_ONTEMPLATE = new Relationship.Builder().name("template_received")
            .description("Any FlowFile that is successfully parsed as a netflowv5 template will be transferred to this Relationship.").build();
    static final Relationship REL_ONDATA = new Relationship.Builder().name("data_received")
            .description("Any FlowFile that is successfully parsed as a netflowv5 data will be transferred to this Relationship.").build();

    @Override
    public Set<Relationship> getRelationships() {
        final Set<Relationship> relationships = new HashSet<Relationship>();
        relationships.add(REL_FAILURE);
        relationships.add(REL_ONTEMPLATE);
        relationships.add(REL_ONDATA);
        return relationships;
    }

    @Override
    public final List<PropertyDescriptor> getSupportedPropertyDescriptors() {
        final List<PropertyDescriptor> descriptors = new ArrayList<PropertyDescriptor>();
        descriptors.add(FIELDS_DESTINATION);
        descriptors.add(APPEND_RAW_MESSAGE_TO_JSON);
        return descriptors;
    }

    @OnScheduled
    public void onScheduled(final ProcessContext context) {
        append_raw_to_json = context.getProperty(APPEND_RAW_MESSAGE_TO_JSON).asBoolean();
        destination = context.getProperty(FIELDS_DESTINATION).getValue();
    }

    @Override
    public void onTrigger(final ProcessContext context, final ProcessSession session) throws ProcessException {
        FlowFile flowFile = session.get();
        if (flowFile == null) {
            return;
        }

        final OptionalInt portNumber = resolvePort(flowFile);
        final Netflowv5Parser parser = new Netflowv5Parser(portNumber);

        final byte[] buffer = new byte[(int) flowFile.getSize()];
        session.read(flowFile, new InputStreamCallback() {

            @Override
            public void process(final InputStream in) throws IOException {
                StreamUtils.fillBuffer(in, buffer);
            }
        });

        final int processedRecord;
        try {
            processedRecord = parser.parse(buffer);
            getLogger().debug("Parsed {} records from the packet", new Object[] { processedRecord });
        } catch (Throwable e) {
            getLogger().error("Parser returned unexpected Exception {} while processing {}; routing to failure", new Object[] { e, flowFile });
            session.transfer(flowFile, REL_FAILURE);
            return;
        }

        try {
            // Transfer one-time-template
            if (isFirstRun.getAndSet(false)) {
                FlowFile templateFlowFile = session.clone(flowFile);
                // Get Template Snapshot
                final ObjectNode results = getTemplate();
                templateFlowFile = session.write(templateFlowFile, new OutputStreamCallback() {
                    @Override
                    public void process(OutputStream out) throws IOException {
                        try (OutputStream outputStream = new BufferedOutputStream(out)) {
                            outputStream.write(mapper.writeValueAsBytes(results));
                        }
                    }
                });

                templateFlowFile = session.putAttribute(templateFlowFile, "templateDefinition", mapper.writeValueAsString(results));
                // Adjust the FlowFile mime.type attribute
                templateFlowFile = session.putAttribute(templateFlowFile, CoreAttributes.MIME_TYPE.key(), "application/json");
                session.transfer(templateFlowFile, REL_ONTEMPLATE);
            }

            final List<FlowFile> multipleRecords = new ArrayList<>();
            switch (destination) {
            case DESTINATION_ATTRIBUTES:
                final Map<String, String> attributes = new HashMap<>();
                generateKV(multipleRecords, session, flowFile, attributes, parser, processedRecord);
                break;
            case DESTINATION_CONTENT:
                generateJSON(multipleRecords, session, flowFile, parser, processedRecord, buffer);
                break;
            }
            // Ready to transfer to success and commit
            session.transfer(multipleRecords, REL_ONDATA);
            session.adjustCounter("Records Processed", processedRecord, false);
            session.commit();
        } catch (Exception e) {
            // The flowfile has failed parsing & validation, routing to failure
            getLogger().error("Failed to parse {} as a netflowv5 message due to {}; routing to failure", new Object[] { flowFile, e });
            // Create a provenance event recording the routing to failure
            session.getProvenanceReporter().route(flowFile, REL_FAILURE);
            session.transfer(flowFile, REL_FAILURE);
            session.commit();
            return;
        } finally {
            session.rollback();
        }
    }

    private void generateJSON(final List<FlowFile> multipleRecords, final ProcessSession session, final FlowFile flowFile, final Netflowv5Parser parser, final int processedRecord, final byte[] buffer)
            throws JsonProcessingException {
        int numberOfRecords = processedRecord;
        FlowFile recordFlowFile = flowFile;
        int record = 0;
        while (numberOfRecords-- > 0) {
            ObjectNode results = mapper.createObjectNode();
            // Add Port number and templateID
            results.set("port", mapper.valueToTree(parser.getPortNumber()));
            results.set("templateID", mapper.valueToTree(templateID));

            // Clone Flowfile if more than one record present
            if (Math.abs(processedRecord - numberOfRecords) > 1) {
                recordFlowFile = session.clone(flowFile);
            }

            // Add JSON Objects
            generateJSONUtil(results, parser, record++);

            // Add original packet content to the resulting JSON
            if (append_raw_to_json) {
                results.set("_raw", mapper.valueToTree(new String(buffer)));
            }

            recordFlowFile = session.write(recordFlowFile, new OutputStreamCallback() {
                @Override
                public void process(OutputStream out) throws IOException {
                    try (OutputStream outputStream = new BufferedOutputStream(out)) {
                        outputStream.write(mapper.writeValueAsBytes(results));
                    }
                }
            });
            // Adjust the FlowFile mime.type attribute
            recordFlowFile = session.putAttribute(recordFlowFile, CoreAttributes.MIME_TYPE.key(), "application/json");
            // Update the provenance for good measure
            session.getProvenanceReporter().modifyContent(recordFlowFile, "Replaced content with parsed netflowv5 fields and values");
            multipleRecords.add(recordFlowFile);
        }
    }

    private void generateKV(final List<FlowFile> multipleRecords, final ProcessSession session, final FlowFile flowFile, final Map<String, String> attributes, final Netflowv5Parser parser,
            final int processedRecord) {
        int numberOfRecords = processedRecord;
        generateHeaderAttributes(attributes, parser);

        final String[] fieldname = getRecordFields();
        int record = 0;
        FlowFile recordFlowFile = flowFile;
        while (numberOfRecords-- > 0) {
            // Process KVs of the Flow Record fields
            final String[] fieldvalue = parser.getRecordData()[record++];
            for (int i = 0; i < fieldname.length; i++) {
                attributes.put("netflowv5.record." + fieldname[i], fieldvalue[i]);
            }

            if (Math.abs(processedRecord - numberOfRecords) > 1)
                recordFlowFile = session.clone(flowFile);

            recordFlowFile = session.putAllAttributes(recordFlowFile, attributes);
            multipleRecords.add(recordFlowFile);
        }
    }

    private OptionalInt resolvePort(final FlowFile flowFile) {
        final String port;
        if ((port = flowFile.getAttribute("udp.port")) != null)
            return OptionalInt.of(Integer.parseInt(port));
        return OptionalInt.empty();
    }

    private void generateJSONUtil(final ObjectNode results, final Netflowv5Parser parser, final int record) {
        final ObjectNode header = mapper.createObjectNode();

        // Process KVs of the Flow Header fields
        String fieldname[] = getHeaderFields();
        String fieldvalue[] = parser.getHeaderData();
        for (int i = 0; i < fieldname.length; i++) {
            header.set(fieldname[i], mapper.valueToTree(fieldvalue[i]));
        }
        results.set("header", header);

        final ObjectNode data = mapper.createObjectNode();
        // Process KVs of the Flow Record fields
        fieldname = getRecordFields();
        fieldvalue = parser.getRecordData()[record];
        for (int i = 0; i < fieldname.length; i++) {
            data.set(fieldname[i], mapper.valueToTree(fieldvalue[i]));
        }
        results.set("record", data);
    }

    private void generateHeaderAttributes(final Map<String, String> attributes, final Netflowv5Parser parser) {
        // Process KVs of the Flow Header fields
        final String[] hFieldName = getHeaderFields();
        final String[] hFieldValue = parser.getHeaderData();

        attributes.put("templateID", String.valueOf(templateID));

        for (int i = 0; i < hFieldName.length; i++) {
            attributes.put("netflowv5.header." + hFieldName[i], hFieldValue[i]);
        }
    }
}